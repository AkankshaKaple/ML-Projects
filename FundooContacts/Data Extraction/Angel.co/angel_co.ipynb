{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tech crunch : \n",
    "    - __Fundings__         -\n",
    "* Snow : \n",
    "    - __Contact details__  -\n",
    "* angel.co : \n",
    "    - __jobs__            - Done\n",
    "    - __tech_req__        - Done\n",
    "    - __compensation__    - Done\n",
    "    - __number_of_emp__   - Done\n",
    "    - __location__        - Done\n",
    "    - __area of interest__- Done\n",
    "    - __Investors__       - Done\n",
    "    - __Founder__         - Done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cfscrape\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from googlesearch import search \n",
    "\n",
    "def get_urls(comapny_name):\n",
    "    sites = ['LinkedIn', 'angel.co jobs', 'techcrunch', 'snow data']\n",
    "    urls = {}\n",
    "    for site in sites:\n",
    "        query = comapny_name + site\n",
    "\n",
    "        url_generator = search(query, tld=\"com\", num=1, stop=1, pause=2)\n",
    "        for url in url_generator:\n",
    "            urls[site] = url\n",
    "    return urls\n",
    "\n",
    "def break_protection(link):\n",
    "    cfscrape.DEFAULT_CIPHERS = 'TLS_AES_256_GCM_SHA384:ECDHE-ECDSA-AES256-SHA384'\n",
    "    scraper = cfscrape.create_scraper()\n",
    "    data = scraper.get(link).content\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_angel_co_data(soup):\n",
    "    links = []\n",
    "    for link in soup.findAll('a', attrs={'href': re.compile(\"^https://\")}):\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "    jobs = soup.find_all(class_='u-fontSize18')\n",
    "    job = [i.text for i in jobs if i.text != '']\n",
    "    all_job_categories = [i for i in job if '\\n' in i]\n",
    "\n",
    "    # Get jobs\n",
    "    dict_1 = {}\n",
    "    value = []\n",
    "    for i in range(len(all_job_categories)):\n",
    "\n",
    "        if all_job_categories[i].strip() == 'Software Engineering Jobs':\n",
    "            value.extend(job[job.index(all_job_categories[i])+1 : job.index(all_job_categories[i+1])])\n",
    "\n",
    "        elif all_job_categories[i].strip() == 'Other Jobs':\n",
    "            value.extend(job[job.index(all_job_categories[i])+1:])\n",
    "\n",
    "    dict_1['jobs'] = value\n",
    "\n",
    "    # Get links\n",
    "    check_list = []\n",
    "    for name in value:\n",
    "        val = \" \".join(name.lower().replace('-','').strip().split()).replace(' ','-').replace('(','').replace(')','')\n",
    "        check_list.append(val.replace('.',''))\n",
    "\n",
    "    job_links = []\n",
    "    for name in check_list:\n",
    "        job_links.extend([i for i in links if name in i])\n",
    "    job_links = list(set(job_links))\n",
    "\n",
    "    # Go to each link and get tech_requirments, employee number and compensation\n",
    "    tech_req = []\n",
    "    Compensation = []\n",
    "    c = 0\n",
    "    for link in job_links:\n",
    "        scraper_1 = cfscrape.create_scraper()\n",
    "        data_1 = scraper_1.get(link).content\n",
    "        soup_1 = BeautifulSoup(data_1, 'html.parser')\n",
    "\n",
    "        inf = soup_1.find_all(class_='s-grid--preMd1')\n",
    "        list_info = [i.text.strip().split('\\n') for i in inf]\n",
    "\n",
    "        for_tech_req = [i for i in list_info[0] if i != '']\n",
    "        for_comp_info = [i for i in list_info[1] if i != '']\n",
    "\n",
    "        if 'Skills' in for_tech_req and c != 0:\n",
    "            tech_req.extend(tech_req + for_tech_req[for_tech_req.index('Skills')+1 : for_tech_req.index('Compensation')])\n",
    "        elif 'Skills' in for_tech_req:\n",
    "            tech_req.extend(for_tech_req[for_tech_req.index('Skills')+1 : for_tech_req.index('Compensation')])\n",
    "        c += 1\n",
    "        if 'Compensation' in for_tech_req:\n",
    "            Compensation.extend([for_tech_req[for_tech_req.index('Compensation')+1]])\n",
    "\n",
    "        key = 'Employees'\n",
    "        if key not in dict_1.keys():\n",
    "            dict_1['Employees'] = [i.replace(' Employees','') for i in for_comp_info if 'Employees' in i]\n",
    "\n",
    "    dict_1['Compensation'] = Compensation\n",
    "    dict_1['tech_req'] = set(tech_req)\n",
    "    \n",
    "    # Get location and area of interest\n",
    "    soup_1 = break_protection(urls['angel.co jobs'].replace('/jobs', ''))\n",
    "    info = [i.text for i in soup_1.findAll(class_='tag')]\n",
    "    dict_1['Location'] = info[0]\n",
    "    dict_1['Area of interest'] = info[1:]\n",
    "    \n",
    "    #Get inverstors\n",
    "    dict_1['investors'] = [i.text for i in soup_1.findAll(class_='startup-link') if i.text != '' ]\n",
    "    \n",
    "    # GetFounders\n",
    "    founders = [i.text.strip().split('\\n\\n\\n') for i in soup_1.findAll(class_='founders section') if i.text.strip().split('\\n\\n\\n') != '' ]\n",
    "    founders_list = [j for j in founders[0] if j != '']\n",
    "    for i in range(len(founders_list)):\n",
    "        dict_1['Founder_' + str(i)] = founders_list[i]\n",
    "    return dict_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "comapny_name = 'Think Analytics'\n",
    "urls = get_urls(comapny_name)\n",
    "soup = break_protection(urls['angel.co jobs'])\n",
    "dict_1 = get_angel_co_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobs': ['Frontend Associate - HTML/CSS/Javascript',\n",
       "  'Backend/Programmer Associate - Python/Java',\n",
       "  'Hybrid Mobile Development',\n",
       "  'Native Android Developer',\n",
       "  'Senior Azure Cloud Solution Architect'],\n",
       " 'Employees': ['51-200'],\n",
       " 'Compensation': ['₹4L – ₹7L', '₹5L – ₹8L', '₹12L – ₹15L'],\n",
       " 'tech_req': {'Android, SQLite, Android SDK',\n",
       "  'Microsoft Azure',\n",
       "  'UI/UX Design, Cordova, Ionic Framework'},\n",
       " 'Location': 'Mumbai',\n",
       " 'Area of interest': ['Analytics',\n",
       "  'Big Data Analytics',\n",
       "  'Business Analytics',\n",
       "  'Predictive Analytics'],\n",
       " 'investors': [],\n",
       " 'Founder_0': 'Suryadip Ghoshal\\n\\nFounder\\nFounder @Think Analytics • Worked at @pwc-consulting',\n",
       " 'Founder_1': 'Amit Das\\n\\nFounder'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
